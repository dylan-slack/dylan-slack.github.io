
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<!-- style from view-source:http://web.simmons.edu/~grabiner/comm244/weekfour/code-test.html#:~:text=You%20can%20include%20code%20examples,code%20as%20what%20it%20is. -->

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

  <title>Backpropagation</title>

  <script src="https://cdn.jsdelivr.net/npm/markdown-it@12.0.0/dist/markdown-it.min.js"></script>

    <script type="text/javascript" id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
  
  <style type="text/css" media="screen">
  
    body {
      line-height: 140%;
      margin: 50px;
      width: 1024px;
    }
    code {font-size: 120%;}
    
    
    pre code {
      background-color: #eee;
      border: 1px solid #999;
      display: block;
      padding: 20px;
    }

    gpt {
      background-color: #eee;
      border: 2px solid #999;
      display: block;
      padding: 10px;
    }
    
  </style>
  
</head>

<body>

<a href="index.html">Return to Index</a>
  
<h1 id="using_code_example_in_html">Backpropagation</h1>

<p> Backpropagation is a fundemental algorithm for computing gradients in deep neural networks. Much effort has been spilled created backpropagation tutorials but let's go over it again. </p>

<h2> Overview </h2>

<p> With a deep neural network, we typically have a series of operations that are performed in intermediate steps, starting at some inputs and arriving at some cost function. When, we update the weights of the network, we want to figure out how the individual weights influence the cost: for instance, does marginally increasing the weight increase or decrease the cost? We can then use this information to update the network. It turns out, that when we put together many layers, with lots of weights, and update the the weights to minimize cost using lots of data, we can train good machine learning models, which is somewhat miraculous. </p>

<p> Let's say we have some model with a cost function C and inputs X. Backpropagation arises from the observation that we can break down intermediate gradient updates via the chain rule, as </p>

<p>$$\frac{\partial C}{\partial X}=\frac{\partial C}{\partial f_n} \frac{\partial f_{n}}{\partial f_{n-1}} \frac{\partial f_{n-1}}{\partial f_{n-2}} \times ... \times \frac{\partial f_0}{\partial X} $$</p>

<p> This fact is quite convenient if we're interesting in differentiating with respect to a fixed cost function C, which is often the case for deep learning. Thus, to compute gradients for any particular operation, we only need the inputs to that operation, to compute $$\frac{\partial f_i}{\partial f_j}$$ and the gradients which follow this operation, which we can compute starting from the output layer and working backwards (backpropagation... get it?). </p>

<h2> Examples </h2>

Let's say we have a series of operations, similar to how we might structure a neural network, given as 

<p> $$a(x) = x^2 $$ </p>
<p> $$b(x, y) = x - y $$ </p>
<p> $$c(x, y) = \frac{1}{2}(x + y) $$ </p>

Which can be written in Numpy as

<pre>
    <code>
def f_a(x):
    return x ** 2

def f_b(x, y):
    return x - y

def f_c(x, y):
    return 0.5 * (x + y)
    </code>
</pre>

We can define "backwards" operations for each of these functions. These backwards operations define how we use the information from the previous step to compute the derivative at the current step and how to combine the derivates from from prior steps with the current derivative operation. For example, we can write each of these functions for a, b, and c as follows.

<pre>
    <code>
# Backwards functions accept input to function 
# and gradients accumulated from before    
def b_f_a(x, grads):
    o = 2 * x
    grad_out = grads * o
    return grad_out

def b_f_b(x, y, grads):
    o_1, o_2 = 1, -1
    return o_1 * grads, o_2 * grads

def b_f_c(x, grads):
    return 0.5 * grads

# Testing it out

a, b = 5, 10

b_out = f_b(a, b)
c_out = f_c(b_out)
a_out = f_a(c_out)
print(a_out) # 6.25

# dO / d_b_in
d_a_d_c = b_f_a(c_out, 1.) # initial gradients are 1, dO/dO = 1.
d_c_d_b = b_f_c(b_out, d_a_d_c)
d_b_d_b_in = b_f_b(a, b, d_c_d_b)
print(d_b_d_b_in) # (-2.5, 2.5)

# Verify against manual calculation
# ( 0.5 (a - b) ) ** 2 
# d/da = 2 * (0.5 (a - b )) (0.5) => 0.5 (a - b)
assert d_b_d_b_in[0] == (0.5 * (a - b)), f"{0.5 * (a - b)}" # True
    </code>
</pre>

<p> To accumulate the gradients, we start with 1 (dO/dO = 1) and pass the inputs and previous gradients into the function. We recursively step through each of the functions to calculate the final quantity. </p>

<h2> Implementations </h2>

<p> Let's do some implementations </p>

Here we have a backprop function for matrix multiplications, where the matrix multiplication is then provided into some function f.

<h3> Matrix multiplication </h3>

<pre>
    <code>
def matmul_forward(A, B):

    return A @ B

def matmul_back(A, B, grads):

    # A : (a, b)
    # B : (b, c)
    # grads : (a, c)

    return grads @ B.T, A.T @ grads
    </code>
</pre>


Why is this the case? Let's consider that,

<p> $$ C = AB $$ </p>
<p> $$ f(C) = J $$ </p>

and we want to compute 

<p> $$ \frac{d J}{d A} $$ </p>

then we have

<p> $$ dC = A \; dB + dA \; B $$ </p>
<p> $$ \frac{d J}{d C} = Q $$ </p>
<p> $$ d J = < Q , dC > $$ </p>

Where < . > refers to the frobenius inner product (dot product equivilent for matrices). Then it follows that 

<p> $$ d J = < Q , A dB + dA B > $$ </p>
<p> $$ d J = < Q , dA B > $$ </p>
<p> $$ d J = <  Q B^T , dA  > $$ </p>
<p> $$ \frac{d J} {dA} = B^T Q $$ </p>

<h3> Sorting </h3>

This is somewhat non standard I suppose, but I'm curious how we can handle sorting in a general backprop function. To make it a bit more interesting, we'll write a function to compute gradients for the top k indices along the first axis in a matrix.


<pre>
    <code>
# Gradients for mean of largest top_k indices

def sort_forward(a):
    # assuming a is shape (B, N) and sorting along first dimension
    argsort = np.argsort(a, axis=1)
    resort = np.argsort(argsort, axis=1)
    def sort_backward(inputs, gradients):
        # gradients will be sorted, so map back
        gradients = np.take_along_axis(gradients, resort, axis=1)
        return gradients
    return np.take_along_axis(a, argsort, axis=1), sort_backward

def slice_last_k(a, k: int = 4):
    def slice_backward(inputs, gradients):
        grads = np.zeros_like(inputs) * 1.
        grads[:, -k:] = gradients
        return grads
    return a[:, -k:], slice_backward

def mean(a):
    def mean_backward(inputs, gradients):
        # (a_0 + a_1 + ... + a_n) / n  => d_i/dx = 1/n
        return gradients[:, None] * (np.ones_like(a) / a.shape[1])
    return np.mean(a, axis=1), mean_backward

inputs = np.array(
    [[3, 1, 9, 10, 3, 1],
     [3, 3, 3, 3, 3, 4]]
)

sort_out, sort_b = sort_forward(inputs)
slice_out, slic_b = slice_last_k(sort_out)
mean_out, mean_b = mean(slice_out)

s = np.ones_like(mean_out)
b = mean_b(slice_out, s)
sl = slic_b(sort_out, b)
f = sort_b(inputs, sl)
print(inputs)
# [[ 3  1  9 10  3  1]
#  [ 3  3  3  3  3  4]]
print(f)
# [[0.25 0.   0.25 0.25 0.25 0.  ]
#  [0.   0.   0.25 0.25 0.25 0.25]]
 </code>
</pre>

We can see the answer matches the intuition that gradient of the output with respect to the input, if we take the top k indices (here k is 4), is 1 / k.

<h3> Neural Network </h3>

Next, let's input gradient descent for a fully connected feedforward neural network via backpropagation for a binary classification problem. I'll need the matmul_forward and matmul_backward function from before. I'll additionally add code for a ReLU non-linearity:

<pre>
    <code>
def relu(a):
    mask = a < 0 # goes to zero
    def relu_backward(inputs, gradients):
        return gradients * (mask * 1.) # gradients go to zero
    a[mask] = 0 # nonlinearity
    return a, relu_backward
    </code>
</pre>


The sigmoid function

<pre>
    <code>
def sig(x):
    return 1. / (1. + np.exp(-1. * x))

def d_sig(x):
    return sig(x) * (1 - sig(x))

def sigmoid(a):
    def sig_back(inputs, gradients):
        sig_grads = d_sig(inputs)
        return sig_grads * gradients
    return sig(a), sig_back
    </code>
</pre>

The cross entropy loss (just defined for two labels).

<pre>
    <code>
def cost(a, t):
    def cost_back(a, t):
        # assuming input grad is always one, because I'll always use this as the last node
        d = (t / (a + 1e-32)) - ((1 - t) / (1 - a + 1e-32)) # adding 1e-32 for numerical stability
        d *= -1. / a.size
        return d
    return -1 * (t * np.log(a + 1e-32) + (1 - t) * np.log(1 - a + 1e-32)).mean(), cost_back
    </code>
</pre>

The arises from 

<p> $$ C(a, t) = t \times log(a) + (1 - t) \times log(1 - a) $$ </p>
<p> $$ \frac{dC}{da} = \frac{t}{a} - \frac{1 - t}{1-a} $$ </p>

Then we also need some data. Here, we set y=1 when x[0] is greater than x[1] cubed.

<pre>
    <code>
data = np.random.normal(size=(400, 2))

y = (data[:, 0] > (data[:, 1] ** 3)) * 1.
y = y[:, None]

train_x, train_y = data[:600], y[:600]
test_x, test_y = data[600:], y[600:]
    </code>
</pre>

Let's make this a neural network with one hidden layer and 1 ReLU activation:

<pre>
    <code>
w1 = np.random.normal(size=(2, 300))
w2 = np.random.normal(size=(300, 300))
final_layer = np.random.normal(size=(300, 1))
    </code>
</pre>  

Write the forward and backward update (sorry kinda ugly)

<pre>
    <code>
def forward_backward(d, y, w1, w2, final_layer, lr=1e-2, i=0, split='train'):

    # forward
    l1_o = matmul_f(d, w1)
    r_l1_o, b_l1_o = relu(l1_o)
    l2_o = matmul_f(r_l1_o, w2)
    # r_l2_o, b_l2_o = relu(l2_o)
    f_o = matmul_f(l2_o, final_layer)
    sig_o, b_sig_o = sigmoid(f_o)
    cost_o, b_cost_o = cost(sig_o, y)

    # backward
    d_cost_d_sig = b_cost_o(sig_o, y)
    d_sig_d_f = b_sig_o(f_o, d_cost_d_sig)
    d_f_d_r_l2_o, d_f_d_wf = matmul_b(l2_o, final_layer, d_sig_d_f)
    final_layer = final_layer - lr * d_f_d_wf
    d_l2_o_d_l1_o, d_w2_d_l1_o = matmul_b(l1_o, w2, d_f_d_r_l2_o)
    w2 = w2 - lr * d_w2_d_l1_o
    d_relu_o_d_l1_0 = b_l1_o(l1_o, d_l2_o_d_l1_o)
    _, d_l1_o_d_w1 = matmul_b(d, w1, d_relu_o_d_l1_0)
    w1 = w1 - lr * d_l1_o_d_w1

    if i % 100 == 0 and split == "test":
        print(cost_o)
        acc = ((sig_o > 0.5) * 1. == y).mean()
        print(acc)

    return w1, w2, final_layer
    </code>
</pre>

And finally the training loop:

<pre>
    <code>
for i in range(10_000):
    w1, w2, final_layer = forward_backward(train_x, train_y, w1, w2, final_layer, 2e-3, i)
    if i % 500 == 0:
        forward_backward(test_x, test_y, w1, w2, final_layer, 2e-3, i, 'test')
    </code>
</pre>

Viola! Running this we get the following. It seems to be working pretty well.

<pre>
    <code>
train 0
30.359810336142232
test 0
26.38590703349299
train 500
1.8078451420111514
test 500
1.2956674072103473
train 1000
1.2789701782351415
test 1000
0.8467361568147035
train 1500
0.8664831663454733
test 1500
0.7404769391499721
train 2000
0.5003729296634499
test 2000
0.4595756090306569
train 2500
0.4396815466267847
test 2500
0.4191157612987989
train 3000
0.3965716690490343
test 3000
0.39292556998419825
train 3500
0.35575861585154545
test 3500
0.3684635935317357
train 4000
0.3174784150549549
test 4000
0.34614141140016075
train 4500
0.2826175465401763
test 4500
0.3259280685579782
train 5000
0.2525727351443503
test 5000
0.3080585654459733
train 5500
0.22782889125787098
test 5500
0.293202163981394
train 6000
0.20962974266967588
test 6000
0.28298606934800596
train 6500
0.19924743449302135
test 6500
0.278466098146375
train 7000
0.1935763197738495
test 7000
0.2758745575809439
train 7500
0.18999981325811546
test 7500
0.27281620572206616
train 8000
0.18743312530602144
test 8000
0.26910717665197126
train 8500
0.1854367711765481
test 8500
0.2651330349386153
train 9000
0.18380615318586513
test 9000
0.26121451623110203
train 9500
0.18242753338860884
test 9500
0.2575234396109889
    </code>
</pre>
